
@article{muthen_bayesian_2012,
	title = {Bayesian {SEM}: {A} more ﬂexible representation of substantive theory},
	doi = {10.1037/a0026802},
	language = {en},
	author = {Muthen, Bengt and Asparouhov, Tihomir},
	year = {2012},
	pages = {78},
	file = {Muthen and Asparouhov - Bayesian SEM A more ﬂexible representation of sub.pdf:/home/michi/Zotero/storage/2YF49DGA/Muthen and Asparouhov - Bayesian SEM A more ﬂexible representation of sub.pdf:application/pdf},
}

@article{jacobucci_regularized_2016,
	title = {Regularized {Structural} {Equation} {Modeling}},
	volume = {23},
	issn = {1070-5511, 1532-8007},
	url = {http://www.tandfonline.com/doi/full/10.1080/10705511.2016.1154793},
	doi = {10.1080/10705511.2016.1154793},
	abstract = {A new method is proposed that extends the use of regularization in both lasso and ridge regression to structural equation models. The method is termed regularized structural equation modeling (RegSEM). RegSEM penalizes speciﬁc parameters in structural equation models, with the goal of creating easier to understand and simpler models. Although regularization has gained wide adoption in regression, very little has transferred to models with latent variables. By adding penalties to speciﬁc parameters in a structural equation model, researchers have a high level of ﬂexibility in reducing model complexity, overcoming poor ﬁtting models, and the creation of models that are more likely to generalize to new samples. The proposed method was evaluated through a simulation study, two illustrative examples involving a measurement model, and one empirical example involving the structural part of the model to demonstrate RegSEM’s utility.},
	language = {en},
	number = {4},
	urldate = {2021-06-03},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Jacobucci, Ross and Grimm, Kevin J. and McArdle, John J.},
	month = jul,
	year = {2016},
	pages = {555--566},
	file = {Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf:/home/michi/Zotero/storage/SFDUNLZN/Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf:application/pdf},
}

@article{van_erp_shrinkage_2019,
	title = {Shrinkage priors for {Bayesian} penalized regression},
	volume = {89},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249618300567},
	doi = {10.1016/j.jmp.2018.12.004},
	abstract = {In linear regression problems with many predictors, penalized regression techniques are often used to guard against overfitting and to select variables relevant for predicting an outcome variable. Recently, Bayesian penalization is becoming increasingly popular in which the prior distribution performs a function similar to that of the penalty term in classical penalization. Specifically, the so-called shrinkage priors in Bayesian penalization aim to shrink small effects to zero while maintaining true large effects. Compared to classical penalization techniques, Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered. However, many different shrinkage priors exist and the available, often quite technical, literature primarily focuses on presenting one shrinkage prior and often provides comparisons with only one or two other shrinkage priors. This can make it difficult for researchers to navigate through the many prior options and choose a shrinkage prior for the problem at hand. Therefore, the aim of this paper is to provide a comprehensive overview of the literature on Bayesian penalization. We provide a theoretical and conceptual comparison of nine different shrinkage priors and parametrize the priors, if possible, in terms of scale mixture of normal distributions to facilitate comparisons. We illustrate different characteristics and behaviors of the shrinkage priors and compare their performance in terms of prediction and variable selection in a simulation study. Additionally, we provide two empirical examples to illustrate the application of Bayesian penalization. Finally, an R package bayesreg is available online (https://github.com/sara-vanerp/bayesreg) which allows researchers to perform Bayesian penalized regression with novel shrinkage priors in an easy manner.},
	language = {en},
	urldate = {2021-06-03},
	journal = {Journal of Mathematical Psychology},
	author = {Van Erp, Sara and Oberski, Daniel L. and Mulder, Joris},
	month = apr,
	year = {2019},
	pages = {31--50},
	file = {van Erp et al. - 2019 - Shrinkage priors for Bayesian penalized regression.pdf:/home/michi/Zotero/storage/4S7YATAQ/van Erp et al. - 2019 - Shrinkage priors for Bayesian penalized regression.pdf:application/pdf},
}

@article{lu_bayesian_2016,
	title = {Bayesian {Factor} {Analysis} as a {Variable}-{Selection} {Problem}: {Alternative} {Priors} and {Consequences}},
	volume = {51},
	issn = {0027-3171, 1532-7906},
	shorttitle = {Bayesian {Factor} {Analysis} as a {Variable}-{Selection} {Problem}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2016.1168279},
	doi = {10.1080/00273171.2016.1168279},
	abstract = {Factor analysis is a popular statistical technique for multivariate data analysis. Developments in the structural equation modeling framework have enabled the use of hybrid confirmatory/exploratory approaches in which factor-loading structures can be explored relatively flexibly within a confirmatory factor analysis (CFA) framework. Recently, Muthén \& Asparouhov proposed a Bayesian structural equation modeling (BSEM) approach to explore the presence of cross loadings in CFA models. We show that the issue of determining factor-loading patterns may be formulated as a Bayesian variable selection problem in which Muthén and Asparouhov’s approach can be regarded as a BSEM approach with ridge regression prior (BSEM-RP). We propose another Bayesian approach, denoted herein as the Bayesian structural equation modeling with spike-and-slab prior (BSEM-SSP), which serves as a one-stage alternative to the BSEM-RP. We review the theoretical advantages and disadvantages of both approaches and compare their empirical performance relative to two modification indices-based approaches and exploratory factor analysis with target rotation. A teacher stress scale data set is used to demonstrate our approach.},
	language = {en},
	number = {4},
	urldate = {2021-09-07},
	journal = {Multivariate Behavioral Research},
	author = {Lu, Zhao-Hua and Chow, Sy-Miin and Loken, Eric},
	month = jul,
	year = {2016},
	pages = {519--539},
	file = {Lu et al. - 2016 - Bayesian Factor Analysis as a Variable-Selection P.pdf:/home/michi/Zotero/storage/B6H9T4QI/Lu et al. - 2016 - Bayesian Factor Analysis as a Variable-Selection P.pdf:application/pdf},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459, 1537-274X},
	doi = {10.1198/016214508000000337},
	language = {en},
	number = {482},
	urldate = {2021-09-08},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	pages = {681--686},
	file = {Park and Casella - 2008 - The Bayesian Lasso.pdf:/home/michi/Zotero/storage/63LSCEKS/Park and Casella - 2008 - The Bayesian Lasso.pdf:application/pdf},
}

@article{merkle_efficient_2020,
	title = {Efficient {Bayesian} {Structural} {Equation} {Modeling} in {Stan}},
	url = {http://arxiv.org/abs/2008.07733},
	abstract = {Structural equation models comprise a large class of popular statistical models, including factor analysis models, certain mixed models, and extensions thereof. Model estimation is complicated by the fact that we typically have multiple interdependent response variables and multiple latent variables (which may also be called random effects or hidden variables), often leading to slow and inefficient MCMC samples. In this paper, we describe and illustrate a general, efficient approach to Bayesian SEM estimation in Stan, contrasting it with previous implementations in R package blavaan (Merkle \& Rosseel, 2018). After describing the approaches in detail, we conduct a practical comparison under multiple scenarios. The comparisons show that the new approach is clearly better. We also discuss ways that the approach may be extended to other models that are of interest to psychometricians.},
	urldate = {2021-09-10},
	journal = {arXiv:2008.07733 [stat]},
	author = {Merkle, Edgar C. and Fitzsimmons, Ellen and Uanhoro, James and Goodrich, Ben},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.07733},
	keywords = {Statistics - Computation},
	annote = {Comment: 21 pages, 5 figures},
	file = {arXiv.org Snapshot:/home/michi/Zotero/storage/5MTICTCA/2008.html:text/html;arXiv Fulltext PDF:/home/michi/Zotero/storage/XVTSVW7E/Merkle et al. - 2020 - Efficient Bayesian Structural Equation Modeling in.pdf:application/pdf},
}

@article{carpenter_stan_2017,
	title = {Stan: {A} {Probabilistic} {Programming} {Language}},
	volume = {76},
	copyright = {Copyright (c) 2017 Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, Allen Riddell},
	issn = {1548-7660},
	shorttitle = {Stan},
	doi = {10.18637/jss.v076.i01},
	abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	month = jan,
	year = {2017},
	note = {Number: 1},
	keywords = {algorithmic differentiation, Bayesian inference, probabilistic programming, Stan},
	pages = {1--32},
	file = {Full Text:/home/michi/Zotero/storage/7NPIICKB/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	issn = {2517-6161},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	abstract = {We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1996.tb02080.x},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288},
	file = {Full Text PDF:/home/michi/Zotero/storage/2JLTWQ3R/Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/7WXZE2SW/j.2517-6161.1996.tb02080.html:text/html},
}

@book{schoot_small_2020,
	address = {London New York},
	series = {European {Association} of {Methodology} series},
	title = {Small sample size solutions: a guide for applied researchers and practitioners\$fedited by {Rens} van de {Schoot} and {Milica} {Miočević}},
	isbn = {978-0-367-22189-8 978-0-367-22222-2},
	shorttitle = {Small sample size solutions},
	language = {en},
	publisher = {Routledge},
	editor = {Schoot, Rens van de and Miočević, Milica},
	year = {2020},
	file = {Schoot and Miočević - 2020 - Small sample size solutions a guide for applied r.pdf:/home/michi/Zotero/storage/H24PDG4V/Schoot and Miočević - 2020 - Small sample size solutions a guide for applied r.pdf:application/pdf},
}

@article{carvalho_horseshoe_2010,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	issn = {0006-3444},
	doi = {10.1093/biomet/asq017},
	abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
	number = {2},
	urldate = {2021-09-16},
	journal = {Biometrika},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	year = {2010},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {465--480},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/2VUZWZNY/CARVALHO et al. - 2010 - The horseshoe estimator for sparse signals.pdf:application/pdf},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	doi = {10.1214/17-EJS1337SI},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	pages = {5018--5051},
	file = {Snapshot:/home/michi/Zotero/storage/9THVXYJ3/17-EJS1337SI.html:text/html;Full Text:/home/michi/Zotero/storage/DMBT7QEE/Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf:application/pdf},
}

@article{zhang_criteria_2021,
	title = {Criteria for {Parameter} {Identification} in {Bayesian} {Lasso} {Methods} for {Covariance} {Analysis}: {Comparing} {Rules} for {Thresholding}, \textit{p} -value, and {Credible} {Interval}},
	issn = {1070-5511, 1532-8007},
	shorttitle = {Criteria for {Parameter} {Identification} in {Bayesian} {Lasso} {Methods} for {Covariance} {Analysis}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2021.1945456},
	doi = {10.1080/10705511.2021.1945456},
	abstract = {The lasso is a commonly used regularization method that is increasing used in structural equation models (SEMs). Under the Bayesian framework, lasso is rendered more flexible and readily produces estimates of standard errors and the penalty parameter. However, in practice, it remains unclear what decision rule is appropriate for parameter identification; in other words, determining what size estimate is large enough to be included into the model. The current study compared three decision rules for parameter identifica­ tion – thresholding, p-value, and credible interval in confirmatory factor analysis. Specifically, two distinct parameter spaces were studied: cross-loadings and residual correlations. Results showed that the thresh­ olding rule performed best in balancing power and Type I error rate. Different thresholds for standardized estimates were needed for different conditions. Guidelines for parameter identification and recom­ mended thresholding values were also provided. Results of the current study have the potential to extend to a broad range of SEMs.},
	language = {en},
	urldate = {2021-09-16},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Zhang, Lijin and Pan, Junhao and Ip, Edward Haksing},
	month = aug,
	year = {2021},
	pages = {1--10},
	file = {Zhang et al. - 2021 - Criteria for Parameter Identification in Bayesian .pdf:/home/michi/Zotero/storage/FYK98DKJ/Zhang et al. - 2021 - Criteria for Parameter Identification in Bayesian .pdf:application/pdf},
}

@article{hoerl_ridge_2000,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {42},
	issn = {0040-1706},
	shorttitle = {Ridge {Regression}},
	url = {https://www.jstor.org/stable/1271436},
	doi = {10.2307/1271436},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
	number = {1},
	urldate = {2021-09-16},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {2000},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {80--86},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/N63Z3K8Z/Hoerl and Kennard - 2000 - Ridge Regression Biased Estimation for Nonorthogo.pdf:application/pdf},
}

@article{hsiang_bayesian_1975,
	title = {A {Bayesian} {View} on {Ridge} {Regression}},
	volume = {24},
	issn = {0039-0526},
	url = {https://www.jstor.org/stable/2987923},
	doi = {10.2307/2987923},
	number = {4},
	urldate = {2021-09-16},
	journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
	author = {Hsiang, T. C.},
	year = {1975},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--268},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/4J2U4JX8/Hsiang - 1975 - A Bayesian View on Ridge Regression.pdf:application/pdf},
}

@article{tibshirani_regression_2011,
	title = {Regression shrinkage and selection via the lasso: a retrospective},
	volume = {73},
	issn = {1369-7412},
	shorttitle = {Regression shrinkage and selection via the lasso},
	url = {https://www.jstor.org/stable/41262671},
	abstract = {In the paper I give a brief review of the basic idea and some history and then discuss some developments since the original paper on regression shrinkage and selection via the lasso.},
	number = {3},
	urldate = {2021-09-16},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Tibshirani, Robert},
	year = {2011},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {273--282},
}

@misc{stan_development_team_stan_2021,
	title = {Stan {User} {Guide}},
	url = {https://mc-stan.org/docs/2_27/stan-users-guide-2_27.pdf},
	urldate = {2021-09-24},
	author = {{Stan Development Team}},
	year = {2021},
	file = {stan-users-guide-2_27.pdf:/home/michi/Zotero/storage/CKR2FGXS/stan-users-guide-2_27.pdf:application/pdf},
}

@article{homan_no-u-turn_2014,
	title = {The {No}-{U}-turn sampler: adaptively setting path lengths in {Hamiltonian} {Monte} {Carlo}},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {The {No}-{U}-turn sampler},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size ε and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more effciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter ε on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Homan, Matthew D. and Gelman, Andrew},
	month = jan,
	year = {2014},
	keywords = {Bayesian inference, adaptive Monte Carlo, dual averaging, Hamiltonian Monte Carlo, Markov chain Monte Carlo},
	pages = {1593--1623},
	file = {Full Text PDF:/home/michi/Zotero/storage/D2L7CXTP/Homan and Gelman - 2014 - The No-U-turn sampler adaptively setting path len.pdf:application/pdf},
}

@article{betancourt_conceptual_2018,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	urldate = {2021-10-05},
	journal = {arXiv:1701.02434 [stat]},
	author = {Betancourt, Michael},
	month = jul,
	year = {2018},
	note = {arXiv: 1701.02434},
	keywords = {Statistics - Methodology},
	annote = {Comment: 60 pages, 42 figures},
	file = {arXiv.org Snapshot:/home/michi/Zotero/storage/88ZVPIH5/1701.html:text/html;arXiv Fulltext PDF:/home/michi/Zotero/storage/P694JZY7/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf},
}

@article{ghosh_use_2018,
	title = {On the {Use} of {Cauchy} {Prior} {Distributions} for {Bayesian} {Logistic} {Regression}},
	volume = {13},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-2/On-the-Use-of-Cauchy-Prior-Distributions-for-Bayesian-Logistic/10.1214/17-BA1051.full},
	doi = {10.1214/17-BA1051},
	abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on Pólya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package tglm, available at CRAN. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler (NUTS) in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
	number = {2},
	urldate = {2021-10-02},
	journal = {Bayesian Analysis},
	author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
	month = jun,
	year = {2018},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Markov chain Monte Carlo, binary regression, existence of posterior mean, probit regression, separation, slow mixing},
	pages = {359--383},
	file = {Snapshot:/home/michi/Zotero/storage/EM6XRLTT/17-BA1051.html:text/html;Full Text PDF:/home/michi/Zotero/storage/A9NGHXVS/Ghosh et al. - 2018 - On the Use of Cauchy Prior Distributions for Bayes.pdf:application/pdf},
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
	doi = {10.1002/sim.8086},
	abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
	language = {en},
	number = {11},
	urldate = {2021-10-13},
	journal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
	keywords = {graphics for simulation, Monte Carlo, simulation design, simulation reporting, simulation studies},
	pages = {2074--2102},
	file = {Full Text PDF:/home/michi/Zotero/storage/A2VSNI46/Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/GLNUHHTL/sim.html:text/html},
}

@article{maccallum_model_1992,
	title = {Model modifications in covariance structure analysis: the problem of capitalization on chance},
	volume = {111},
	issn = {0033-2909},
	shorttitle = {Model modifications in covariance structure analysis},
	doi = {10.1037/0033-2909.111.3.490},
	abstract = {In applications of covariance structure modeling in which an initial model does not fit sample data well, it has become common practice to modify that model to improve its fit. Because this process is data driven, it is inherently susceptible to capitalization on chance characteristics of the data, thus raising the question of whether model modifications generalize to other samples or to the population. This issue is discussed in detail and is explored empirically through sampling studies using 2 large sets of data. Results demonstrate that over repeated samples, model modifications may be very inconsistent and cross-validation results may behave erratically. These findings lead to skepticism about generalizability of models resulting from data-driven modifications of an initial model. The use of alternative a priori models is recommended as a preferred strategy.},
	language = {eng},
	number = {3},
	journal = {Psychological Bulletin},
	author = {MacCallum, R. C. and Roznowski, M. and Necowitz, L. B.},
	month = may,
	year = {1992},
	pmid = {16250105},
	keywords = {Adolescent, Algorithms, Data Interpretation, Statistical, Empirical Research, Factor Analysis, Statistical, Humans, Intelligence Tests, Job Satisfaction, Models, Psychological, Models, Statistical, Reproducibility of Results, Sample Size, Sampling Studies, Software, Surveys and Questionnaires},
	pages = {490--504},
}

@misc{noauthor_apa_nodate,
	title = {{APA} {PsycNet}},
	url = {https://psycnet.apa.org/doiLanding?doi=10.1037/0033-2909.111.3.490},
	urldate = {2021-10-14},
}

@article{piironen_projection_2015,
	title = {Projection predictive variable selection using {Stan}+{R}},
	url = {http://arxiv.org/abs/1508.02502},
	abstract = {This document is additional material to our previous study comparing several strategies for variable subset selection. Our recommended approach was to fit the full model with all the candidate variables and best possible prior information, and perform the variable selection using the projection predictive framework. Here we give an example of performing such an analysis, using Stan for fitting the model, and R for the variable selection.},
	urldate = {2021-10-18},
	journal = {arXiv:1508.02502 [stat]},
	author = {Piironen, Juho and Vehtari, Aki},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.02502},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/michi/Zotero/storage/5QIPPCHN/Piironen and Vehtari - 2015 - Projection predictive variable selection using Sta.pdf:application/pdf;arXiv.org Snapshot:/home/michi/Zotero/storage/QAU35K5F/1508.html:text/html},
}

@inproceedings{carvalho_handling_2009,
	title = {Handling {Sparsity} via the {Horseshoe}},
	url = {https://proceedings.mlr.press/v5/carvalho09a.html},
	abstract = {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the {Twelth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	month = apr,
	year = {2009},
	note = {ISSN: 1938-7228},
	pages = {73--80},
	file = {Full Text PDF:/home/michi/Zotero/storage/YCAXJV9K/Carvalho et al. - 2009 - Handling Sparsity via the Horseshoe.pdf:application/pdf},
}

@misc{noauthor_comparison_nodate,
	title = {A {Comparison} of {Bayesian} and {Frequentist} {Model} {Selection} {Methods} for {Factor} {Analysis} {Models}},
	url = {https://oce-ovid-com.proxy.library.uu.nl/article/00060744-201706000-00009/HTML},
	urldate = {2021-11-17},
	file = {A Comparison of Bayesian and Frequentist Model Selection Methods for Factor Analysis Models:/home/michi/Zotero/storage/XTR3F955/HTML.html:text/html},
}

@article{monnahan_faster_2017,
	title = {Faster estimation of {Bayesian} models in ecology using {Hamiltonian} {Monte} {Carlo}},
	volume = {8},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12681},
	doi = {10.1111/2041-210X.12681},
	abstract = {Bayesian inference is a powerful tool to better understand ecological processes across varied subfields in ecology, and is often implemented in generic and flexible software packages such as the widely used BUGS family (BUGS, WinBUGS, OpenBUGS and JAGS). However, some models have prohibitively long run times when implemented in BUGS. A relatively new software platform called Stan uses Hamiltonian Monte Carlo (HMC), a family of Markov chain Monte Carlo (MCMC) algorithms which promise improved efficiency and faster inference relative to those used by BUGS. Stan is gaining traction in many fields as an alternative to BUGS, but adoption has been slow in ecology, likely due in part to the complex nature of HMC. Here, we provide an intuitive illustration of the principles of HMC on a set of simple models. We then compared the relative efficiency of BUGS and Stan using population ecology models that vary in size and complexity. For hierarchical models, we also investigated the effect of an alternative parameterization of random effects, known as non-centering. For small, simple models there is little practical difference between the two platforms, but Stan outperforms BUGS as model size and complexity grows. Stan also performs well for hierarchical models, but is more sensitive to model parameterization than BUGS. Stan may also be more robust to biased inference caused by pathologies, because it produces diagnostic warnings where BUGS provides none. Disadvantages of Stan include an inability to use discrete parameters, more complex diagnostics and a greater requirement for hands-on tuning. Given these results, Stan is a valuable tool for many ecologists utilizing Bayesian inference, particularly for problems where BUGS is prohibitively slow. As such, Stan can extend the boundaries of feasible models for applied problems, leading to better understanding of ecological processes. Fields that would likely benefit include estimation of individual and population growth rates, meta-analyses and cross-system comparisons and spatiotemporal models.},
	language = {en},
	number = {3},
	urldate = {2021-12-14},
	journal = {Methods in Ecology and Evolution},
	author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12681},
	keywords = {Bayesian inference, Stan, Markov chain Monte Carlo, hierarchical modelling, no-U-turn sampler},
	pages = {339--348},
	file = {Full Text PDF:/home/michi/Zotero/storage/LHX3AWCW/Monnahan et al. - 2017 - Faster estimation of Bayesian models in ecology us.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/XP2RFGP6/2041-210X.html:text/html},
}

@article{van_erp_prior_2018,
	title = {Prior sensitivity analysis in default {Bayesian} structural equation modeling},
	volume = {23},
	issn = {1939-1463},
	doi = {10.1037/met0000162},
	abstract = {Bayesian structural equation modeling (BSEM) has recently gained popularity because it enables researchers to fit complex models and solve some of the issues often encountered in classical maximum likelihood estimation, such as nonconvergence and inadmissible solutions. An important component of any Bayesian analysis is the prior distribution of the unknown model parameters. Often, researchers rely on default priors, which are constructed in an automatic fashion without requiring substantive prior information. However, the prior can have a serious influence on the estimation of the model parameters, which affects the mean squared error, bias, coverage rates, and quantiles of the estimates. In this article, we investigate the performance of three different default priors: noninformative improper priors, vague proper priors, and empirical Bayes priors—with the latter being novel in the BSEM literature. Based on a simulation study, we find that these three default BSEM methods may perform very differently, especially with small samples. A careful prior sensitivity analysis is therefore needed when performing a default BSEM analysis. For this purpose, we provide a practical step-by-step guide for practitioners to conducting a prior sensitivity analysis in default BSEM. Our recommendations are illustrated using a well-known case study from the structural equation modeling literature, and all code for conducting the prior sensitivity analysis is available in the online supplemental materials. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Methods},
	author = {van Erp, Sara and Mulder, Joris and Oberski, Daniel L.},
	year = {2018},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Statistical Analysis, Statistical Probability, Structural Equation Modeling},
	pages = {363--388},
	file = {Submitted Version:/home/michi/Zotero/storage/DTLS28IZ/van Erp et al. - 2018 - Prior sensitivity analysis in default Bayesian str.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/DBYTKIUU/2017-52406-001.html:text/html},
}

@book{bollen_structural_1989,
	title = {Structural {Equations} with {Latent} {Variables}},
	isbn = {978-0-471-01171-2},
	abstract = {Analysis of Ordinal Categorical Data Alan Agresti Statistical Science Now has its first coordinated manual of methods for analyzing ordered categorical data. This book discusses specialized models that, unlike standard methods underlying nominal categorical data, efficiently use the information on ordering. It begins with an introduction to basic descriptive and inferential methods for categorical data, and then gives thorough coverage of the most current developments, such as loglinear and logit models for ordinal data. Special emphasis is placed on interpretation and application of methods and contains an integrated comparison of the available strategies for analyzing ordinal data. This is a case study work with illuminating examples taken from across the wide spectrum of ordinal categorical applications. 1984 (0 471-89055-3) 287 pp. Regression Diagnostics Identifying Influential Data and Sources of Collinearity David A. Belsley, Edwin Kuh and Roy E. Welsch This book provides the practicing statistician and econometrician with new tools for assessing the quality and reliability of regression estimates. Diagnostic techniques are developed that aid in the systematic location of data points that are either unusual or inordinately influential; measure the presence and intensity of collinear relations among the regression data and help to identify the variables involved in each; and pinpoint the estimated coefficients that are potentially most adversely affected. The primary emphasis of these contributions is on diagnostics, but suggestions for remedial action are given and illustrated. 1980 (0 471-05856-4) 292 pp. Applied Regression Analysis Second Edition Norman Draper and Harry Smith Featuring a significant expansion of material reflecting recent advances, here is a complete and up-to-date introduction to the fundamentals of regression analysis, focusing on understanding the latest concepts and applications of these methods. The authors thoroughly explore the fitting and checking of both linear and nonlinear regression models, using small or large data sets and pocket or high-speed computing equipment. Features added to this Second Edition include the practical implications of linear regression; the Durbin-Watson test for serial correlation; families of transformations; inverse, ridge, latent root and robust regression; and nonlinear growth models. Includes many new exercises and worked examples. 1981 (0 471-02995-5) 709 pp.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Bollen, Kenneth A.},
	month = may,
	year = {1989},
	note = {Google-Books-ID: 4a3UDwAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@book{cox_principles_2006,
	title = {Principles of {Statistical} {Inference}},
	isbn = {978-1-139-45913-6},
	abstract = {In this definitive book, D. R. Cox gives a comprehensive and balanced appraisal of statistical inference. He develops the key concepts, describing and comparing the main ideas and controversies over foundational issues that have been keenly argued for more than two-hundred years. Continuing a sixty-year career of major contributions to statistical thought, no one is better placed to give this much-needed account of the field. An appendix gives a more personal assessment of the merits of different ideas. The content ranges from the traditional to the contemporary. While specific applications are not treated, the book is strongly motivated by applications across the sciences and associated technologies. The mathematics is kept as elementary as feasible, though previous knowledge of statistics is assumed. The book will be valued by every user or student of statistics who is serious about understanding the uncertainty inherent in conclusions from statistical analyses.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Cox, D. R.},
	month = aug,
	year = {2006},
	note = {Google-Books-ID: nRgtGZXi2KkC},
	keywords = {Mathematics / Probability \& Statistics / General, Business \& Economics / Statistics, Medical / Epidemiology, Social Science / Research},
}

@article{george_variable_1993,
	title = {Variable {Selection} {Via} {Gibbs} {Sampling}},
	volume = {88},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290777},
	doi = {10.2307/2290777},
	abstract = {A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability--the promising ones--can then be identified by their more frequent appearance in the Gibbs sample.},
	number = {423},
	urldate = {2022-04-21},
	journal = {Journal of the American Statistical Association},
	author = {George, Edward I. and McCulloch, Robert E.},
	year = {1993},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {881--889},
}

@article{mitchell_bayesian_1988,
	title = {Bayesian {Variable} {Selection} in {Linear} {Regression}},
	volume = {83},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290129},
	doi = {10.2307/2290129},
	abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose γ. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
	number = {404},
	urldate = {2022-04-21},
	journal = {Journal of the American Statistical Association},
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	year = {1988},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1023--1032},
}

@article{ishwaran_spike_2005,
	title = {Spike and slab variable selection: {Frequentist} and {Bayesian} strategies},
	volume = {33},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Spike and slab variable selection},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-2/Spike-and-slab-variable-selection-Frequentist-and-Bayesian-strategies/10.1214/009053604000001147.full},
	doi = {10.1214/009053604000001147},
	abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure’s ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
	number = {2},
	urldate = {2022-04-21},
	journal = {The Annals of Statistics},
	author = {Ishwaran, Hemant and Rao, J. Sunil},
	month = apr,
	year = {2005},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {shrinkage, 62J05, 62J07, Generalized ridge regression, hypervariance, model averaging, model uncertainty, ordinary least squares, Penalization, rescaling, stochastic variable selection, Zcut},
	pages = {730--773},
	file = {Full Text PDF:/home/michi/Zotero/storage/VERMAD96/Ishwaran and Rao - 2005 - Spike and slab variable selection Frequentist and.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/D3EDH79C/009053604000001147.html:text/html},
}

@book{james_introduction_2021,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {978-1-07-161417-4 978-1-07-161418-1},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	url = {https://link.springer.com/10.1007/978-1-0716-1418-1},
	language = {en},
	urldate = {2022-04-21},
	publisher = {Springer US},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2021},
	doi = {10.1007/978-1-0716-1418-1},
	file = {James et al. - 2021 - An Introduction to Statistical Learning with Appl.pdf:/home/michi/Zotero/storage/R52MEIUV/James et al. - 2021 - An Introduction to Statistical Learning with Appl.pdf:application/pdf},
}

@inproceedings{piironen_hyperprior_2017,
	title = {On the {Hyperprior} {Choice} for the {Global} {Shrinkage} {Parameter} in the {Horseshoe} {Prior}},
	url = {https://proceedings.mlr.press/v54/piironen17a.html},
	abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but as shown in this paper, the results can be sensitive to the prior choice for the global shrinkage hyperparameter. We argue that the previous default choices are dubious due to their tendency to favor solutions with more unshrunk coefficients than we typically expect a priori. This can lead to bad results if this parameter is not strongly identified by data. We derive the relationship between the global parameter and the effective number of nonzeros in the coefficient vector, and show an easy and intuitive way of setting up the prior for the global parameter based on our prior beliefs about the number of nonzero coefficients in the model. The results on real world data show that one can benefit greatly – in terms of improved parameter estimates, prediction accuracy, and reduced computation time – from transforming even a crude guess for the number of nonzero coefficients into the prior for the global parameter using our framework.},
	language = {en},
	urldate = {2022-04-23},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Piironen, Juho and Vehtari, Aki},
	month = apr,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {905--913},
	file = {Full Text PDF:/home/michi/Zotero/storage/P6LSMW7Y/Piironen and Vehtari - 2017 - On the Hyperprior Choice for the Global Shrinkage .pdf:application/pdf;Supplementary PDF:/home/michi/Zotero/storage/KIGACJBM/Piironen and Vehtari - 2017 - On the Hyperprior Choice for the Global Shrinkage .pdf:application/pdf},
}

@article{obrien_statistical_2016,
	title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {Generalizations}: {Book} {Reviews}},
	volume = {84},
	issn = {03067734},
	shorttitle = {Statistical {Learning} with {Sparsity}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12167},
	doi = {10.1111/insr.12167},
	language = {en},
	number = {1},
	urldate = {2022-05-02},
	journal = {International Statistical Review},
	author = {O'Brien, Carl M.},
	month = apr,
	year = {2016},
	pages = {156--157},
	file = {O'Brien - 2016 - Statistical Learning with Sparsity The Lasso and .pdf:/home/michi/Zotero/storage/5UHSC2U4/O'Brien - 2016 - Statistical Learning with Sparsity The Lasso and .pdf:application/pdf},
}

@article{hastie_statistical_2015,
	title = {Statistical learning with sparsity},
	volume = {143},
	journal = {Monographs on statistics and applied probability},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
	pages = {143},
	file = {Full Text:/home/michi/Zotero/storage/NJXGEJIM/Hastie et al. - 2015 - Statistical learning with sparsity.pdf:application/pdf},
}
